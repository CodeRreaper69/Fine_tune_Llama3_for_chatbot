# -*- coding: utf-8 -*-
"""SynBotTest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YU2Rve7gcUXcMkF4eQUvj9FI8jNJXjZi

FOR CHECKING THE COMPATIBILITY WITH THE COMPUTER WITH RESOURCES
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import torch
# major_version, minor_version = torch.cuda.get_device_capability()
# !pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
# if major_version >= 8:
#     !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes
# else:
#     !pip install --no-deps xformers trl peft accelerate bitsandbytes
# pass
# #%%capture
# # Installs Unsloth, Xformers (Flash Attention) and all other packages!
# #!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
# !pip install --no-deps "xformers<0.0.27" "trl<0.9.0" peft accelerate bitsandbytes
#

"""IMPORTANT LIBRARIES FOR THE CODE TO EXECUTE

"""

from transformers import AutoModelForCausalLM, AutoTokenizer
from unsloth import FastLanguageModel
from peft import PeftModel
import torch

"""LOADING THE BASIC MODEL LLAMA 3 MODEL FROM UNSLOTH"""

# Load the base model and tokenizer
base_model_name = "unsloth/llama-3-8b-bnb-4bit"
tokenizer = AutoTokenizer.from_pretrained(base_model_name)
base_model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.float16, low_cpu_mem_usage=True)

"""LOAD THE LORA ADAPTERS ,SINCE I HAVE FINE TUNED THE MODEL EARLIER

"""

from peft import PeftModel

# Load the LoRA adapters
model_name = "GenDey/SynBot_Model"
model = PeftModel.from_pretrained(base_model, model_name, torch_dtype=torch.float16)

"""CHAT/TESTING WITH THE FINE TUNED MODEL AFTER IT GETS FINE TUNED"""

from transformers import TextStreamer
import io
import sys

# Define a function to suppress output
class SuppressOutput:
    def __enter__(self):
        self._stdout = sys.stdout
        self._stderr = sys.stderr
        sys.stdout = io.StringIO()
        sys.stderr = io.StringIO()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stdout = self._stdout
        sys.stderr = self._stderr


# Define a prompt template for SynBot
SynBot_prompt = """You are a Syntalix.AI consultancy chatbot. Below is a scenario describing a visitor's query, paired with some context. Write a response that appropriately addresses the visitor's needs.

### Scenario:
{}

### Context:
{}

### Response:
"""


FastLanguageModel.for_inference(model)
print("\n---WELCOME SYNTALIX CHATBOT SERVICES---\n (Enter q to exit)\n")

# Start the conversation loop
while True:
    Query = input("YOU <*><*>      : ")

    if Query == 'q':
      break
    # Input = input("Wanna add some EXTRAS (this is highly EXPERIMENTAL, leave blank if no or q to exit, generally for more specific query, like some document or pdf upload query):\n ")
    # if Input == 'q':
    #     break
    inputs = tokenizer(
        [
            SynBot_prompt.format(
                f"{Query}",  # instruction or Query
                f"{Input}",  # input or
                "",  # output - leave this blank for generation!
            )
        ], return_tensors="pt").to("cuda")

    with SuppressOutput():  # Suppress the output from model.generate()
        text_streamer = TextStreamer(tokenizer)
        generated_output = model.generate(**inputs, streamer=text_streamer, max_new_tokens=200, use_cache = True)

    # Convert generated tokens back to text
    generated_text = tokenizer.decode(generated_output[0], skip_special_tokens=True)

    # Extract the Response part
    response_start = generated_text.find("### Response:") + len("### Response:")
    response_text = generated_text[response_start:].strip()

    print("\nSynBot <`><`>   :",response_text,"\n")